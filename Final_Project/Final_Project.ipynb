{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a notebook containing a code for a bot deployed using streamlit to answer all your questions regarding ICS and related information. It uses an adaptive RAG to classify your queries and provide you well suited answers based on the query and classification."
      ],
      "metadata": {
        "id": "da6NFk01u-uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following pip install commands to install the required libraries:"
      ],
      "metadata": {
        "id": "NZd-wkXxumvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain_community --quiet\n",
        "!pip install langchain_google_genai --quiet\n",
        "!pip install langchain_text_splitter --quiet\n",
        "!pip install chromadb --quiet\n",
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "id": "1eoD0FBMur-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code webscrapes the ICS website and produces full text by webscraping all the links to which the base url is linked to as well. After this the text is broken into chunks of size 500 with an overlap of 50 to maintain continuity."
      ],
      "metadata": {
        "id": "G9CnC4cTu-Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "\n",
        "# Example URL (replace with your target)\n",
        "base_url = \"https://www.iitk.ac.in/counsel/\"\n",
        "\n",
        "response = requests.get(base_url)\n",
        "\n",
        "total_text = []\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "full_text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "links = set()\n",
        "for a_tag in soup.find_all(\"a\", href=True):\n",
        "    href = a_tag[\"href\"]\n",
        "    full_url = urljoin(base_url, href)\n",
        "\n",
        "    if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
        "        links.add(full_url)\n",
        "total_text = []\n",
        "for link in links:\n",
        "    try:\n",
        "        response = requests.get(link)\n",
        "        time.sleep(1)\n",
        "        page_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        page_text = page_soup.get_text(separator=\"\\n\", strip=True)\n",
        "        total_text.append(page_text)\n",
        "        print(f\"\\nðŸ“„ {link}\")\n",
        "        print(page_text[:100])  # Print first 100 chars\n",
        "    except Exception as e:\n",
        "        print(f\"Couldnt load link : {link}\")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [Document(page_content=text) for text in total_text]\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
        ")\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "l_532xp8uZun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "fI2nuCatvo-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we initialize the embeddings and store the chunks into a vector store with the directory 'my_chroma_db' so it can accessed within the app.py file again without having to create the vector store repeatedly when the app is launched."
      ],
      "metadata": {
        "id": "Qd9MW02nvqP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "texts = [str(chunk.page_content) for chunk in chunks]\n",
        "vectorstore = Chroma.from_texts(texts, embeddings, persist_directory=\"my_chroma_db\")\n",
        "\n",
        "# Persist to disk\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "id": "fQb8qblQuaVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we write the app.py file along with the functions of adaptive RAG."
      ],
      "metadata": {
        "id": "S-gG2m7N1Eyw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKfcUO_I4XBi"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import time\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import requests\n",
        "import numpy as np\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel,Field\n",
        "from typing import List\n",
        "import os\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "vectorstore = Chroma(persist_directory=\"my_chroma_db\",embedding_function=embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\":10})\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature = 0.1,\n",
        "    api_key=GOOGLE_API_KEY,\n",
        ")\n",
        "class categories_options(BaseModel):\n",
        "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
        "prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
        "        )\n",
        "classify_chain = prompt | llm.with_structured_output(categories_options)\n",
        "class SubQueries(BaseModel):\n",
        "    sub_queries: List[str] = Field(description=\"List of sub-queries\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
        "def Analytical(query):\n",
        "  prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Generate 3 sub-questions for: {query}\"\n",
        "        )\n",
        "  chain = prompt | llm.with_structured_output(SubQueries)\n",
        "  new_query = new_query = chain.invoke({\"query\": query}).sub_queries\n",
        "  response_prompt = PromptTemplate(\n",
        "        input_variables=[\"new_query\"],\n",
        "        template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n",
        "                  \"If you don't know the answer, just say that you don't know. \"\n",
        "                  \"Use three sentences maximum and keep the answer concise. \"\n",
        "                  \"Question: {question} Context: {context}Answer:\"\n",
        "    )\n",
        "\n",
        "  chain = response_prompt | llm\n",
        "  for i in new_query:\n",
        "    response = chain.invoke({\"context\" : retriever.get_relevant_documents(i) , \"question\": i}).content\n",
        "    st.write(f\"Query : {i} ; Answer : {response}\\n\")\n",
        "\n",
        "def Factual(query :str):\n",
        "    prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Enhance this factual query for better information retrieval but do not give options it should be a single query \\nQuery: {query}\\nNew_Query:\"\n",
        "        )\n",
        "    new_query = (prompt | llm).invoke({\"query\": query}).content\n",
        "\n",
        "    response_prompt = PromptTemplate(\n",
        "        input_variables=[\"new_query\"],\n",
        "        template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n",
        "                  \"If you don't know the answer, just say that you don't know. \"\n",
        "                  \"Use three sentences maximum and keep the answer concise. \"\n",
        "                  \"Question: {question} Context: {context}Answer:\"\n",
        "    )\n",
        "\n",
        "    chain = response_prompt | llm\n",
        "    response = chain.invoke({\"context\" : retriever.get_relevant_documents(new_query) , \"question\": new_query}).content\n",
        "    st.write(response)\n",
        "def Opinion(query : str):\n",
        "  prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Create 3 new queries explaining different view points in relation to the given query: {query}\"\n",
        "        )\n",
        "  chain = prompt | llm.with_structured_output(SubQueries)\n",
        "  new_query = new_query = chain.invoke({\"query\": query}).sub_queries\n",
        "  response_prompt = PromptTemplate(\n",
        "        input_variables=[\"new_query\"],\n",
        "        template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n",
        "                  \"If you don't know the answer, just say that you don't know. \"\n",
        "                  \"Use three sentences maximum and keep the answer concise. \"\n",
        "                  \"Question: {question} Context: {context}Answer:\"\n",
        "    )\n",
        "  chain = response_prompt | llm\n",
        "  for i in new_query:\n",
        "    response = chain.invoke({\"context\" : retriever.get_relevant_documents(i) , \"question\": i}).content\n",
        "    st.write(f\"Query : {i} ; Answer : {response}\\n\")\n",
        "def Contextual(query):\n",
        "  context_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\"],\n",
        "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
        "        )\n",
        "  context_chain = context_prompt | llm\n",
        "  contextualized_query = context_chain.invoke({\"context\":retriever.get_relevant_documents(query), \"query\" : query}).content\n",
        "  response_prompt = PromptTemplate(\n",
        "        input_variables=[\"new_query\"],\n",
        "        template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n",
        "                  \"If you don't know the answer, just say that you don't know. \"\n",
        "                  \"Use three sentences maximum and keep the answer concise. \"\n",
        "                  \"Question: {question} Context: {context}Answer:\"\n",
        "    )\n",
        "  chain = response_prompt | llm\n",
        "  response = chain.invoke({\"context\" : retriever.get_relevant_documents(contextualized_query) , \"question\": contextualized_query}).content\n",
        "  st.write(response)\n",
        "st.title(\"Hi, this is a bot to help you answer all your questions related to ICS and more!!!\")\n",
        "st.write(\"Welcome! Any question you have can be asked below:\")\n",
        "query = st.text_input(\"Enter your question or topic:\")\n",
        "if query:\n",
        "    category = classify_chain.invoke(query).category\n",
        "    with st.spinner(f\"Processing...\"):\n",
        "      time.sleep(1)\n",
        "    st.write(f\"Your query is a {category} query\")\n",
        "    globals()[category](query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we launch the app to open the app click on the link printed."
      ],
      "metadata": {
        "id": "RyNkSMv21JoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "AUTH = userdata.get(\"NGROK\")\n",
        "ngrok.set_auth_token(AUTH)\n",
        "!pkill streamlit\n",
        "ngrok.kill()\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"ðŸš€ Streamlit is live at: {public_url}\")\n",
        "\n",
        "!streamlit run app.py --server.port 8501 --server.headless true --server.enableCORS false\n"
      ],
      "metadata": {
        "id": "EsKSO3csHICh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}